{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (Word2Vec as Matrix Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you are supposed to apply SVD to training your own [word embedding model](https://en.wikipedia.org/wiki/Word_embedding) which maps English words to vectors of real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram Negative Sampling (SGNS) word embedding model, commonly known as **word2vec** ([Mikolov et al., 2013](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)), is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as implicit matrix factorization objective as was shown in ([Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a text corpus given as a sequence of words $\\{w_1,w_2,\\dots,w_n\\}$ where $n$ may be larger than $10^{12}$ and $w_i \\in \\mathcal{V}$ belongs to a vocabulary of words $\\mathcal{V}$. A word $c \\in \\mathcal{V}$ is called *a context* of word $w_i$ if they are found together in the text. More formally, given some measure $L$ of closeness between two words (typical choice is $L=2$), a word $c \\in \\mathcal{V}$ is called a context if $c \\in \\{w_{i-L}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+L} \\}$ Let $\\mathbf{w},\\mathbf{c}\\in\\mathbb{R}^d$ be the *word embeddings* of word $w$ and context $c$, respectively. Assume they are specified by the mapping  $\\Phi:\\mathcal{V}\\rightarrow\\mathbb{R}^d$, so $\\mathbf{w}=\\Phi(w)$. The ultimate goal of SGNS word embedding model is to fit a good mapping $\\Phi$.\n",
    "\n",
    "Let $\\mathcal{D}$ be a multiset of all word-contexts pairs observed in the corpus. In the SGNS model, the probability that word-context pair $(w,c)$ is observed in the corpus is modeled as the following distribution:\n",
    "$$\n",
    "P(\\#(w,c)\\neq 0|w,c) = \\sigma(\\mathbf{w}^\\top \\mathbf{c}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^\\top \\mathbf{c})},\n",
    "$$\n",
    "where $\\#(w,c)$ is the number of times the pair $(w,c)$ appears in $\\mathcal{D}$ and $\\mathbf{w}^\\top\\mathbf{c}$ is the scalar product of vectors $\\mathbf{w}$ and $\\mathbf{c}$. Two important quantities which we will also use further are the number of times the word $w$ and the context $c$ appear in $\\mathcal{D}$, which can be computed as\n",
    "$$\n",
    "\\#(w) = \\sum_{c\\in\\mathcal{V}} \\#(w,c), \\quad \\#(c) = \\sum_{w\\in\\mathcal{V}} \\#(w,c).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Optimization objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla word embedding models are trained by maximizing log-likelihood of observed word-context pairs, namely\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram Negative Sampling approach modifies the objective by additionally minimizing the log-likelihood of random word-context pairs, so called *negative samples*. This idea incorporates some useful linguistic information that some number ($k$, usually $k=5$) of word-context pairs *are not* found together in the corpus which usually results in word embeddings of higher quality. The resulting optimization problem is\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + k \\cdot \\mathbb{E}_{c'\\sim P_\\mathcal{D}} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "where $P_\\mathcal{D}(c)=\\frac{\\#(c)}{|\\mathcal{D}|}$ is a probability distribution over word contexts from which negative samples are drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) showed that this objective can be equivalently written as\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} f(w,c) = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + \\frac{k\\cdot\\#(w)\\cdot\\#(c)}{|\\mathcal{D}|} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "A crucial observation is that this loss function depends only on the scalar product $\\mathbf{w}^\\top\\mathbf{c}$ but not on embedding $\\mathbf{w}$ and $\\mathbf{c}$ separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Matrix factorization problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote vocabulary size $|\\mathcal{V}|$ as m. Let $W \\in \\mathbb{R}^{|\\mathcal{V}|\\times d}$ and $C \\in \\mathbb{R}^{|\\mathcal{V}|\\times d}$ be matrices, where each row $\\mathbf{w}\\in\\mathbb{R}^d$ of matrix $W$ is the word embedding of the corresponding word $w$ and each row $\\mathbf{c}\\in\\mathbb{R}^d$ of matrix $C$ is the context embedding of the corresponding context $c$. SGNS embeds both words and their contexts into a low-dimensional space $\\mathbb{R}^d$, resulting in the word and context matrices $W$ and $C$. The rows of matrix $W$ are typically used in NLP tasks (such as computing word similarities) while $C$ is ignored. It is nonetheless instructive to consider the product $W^\\top C = M$. Viewed this way, SGNS can be described as factorizing an implicit matrix M of dimensions $m \\times m$ into two smaller matrices.\n",
    "\n",
    "Which matrix is being factorized? A matrix entry $M_{wc}$ corresponds to the dot product $\\mathbf{w}^\\top\\mathbf{c}$ . Thus, SGNS is factorizing a matrix in which each row corresponds to a word $w \\in \\mathcal{V}$ , each column corresponds to a context $c \\in \\mathcal{V}$, and each cell contains a quantity $f(w,c)$ reflecting the strength of association between that particular word-context pair. Such word-context association matrices are very common in the NLP and word-similarity literature. That said, the objective of SGNS does not explicitly state what this association metric is. What can we say about the association function $f(w,c)$? In other words, which matrix is SGNS factorizing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (theoretical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve SGNS optimization problem with respect to the $\\mathbf{w}^\\top\\mathbf{c}$ and show that the matrix being factorized is\n",
    "$$\n",
    "M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} \\right)\n",
    "$$\n",
    "**Hint:** Denote $x=\\mathbf{w}^\\top\\mathbf{c}$, rewrite SGNG optimization problem in terms of $x$ and solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This matrix is called Shifted Pointwise Mutual Information (SPMI) matrix, as its elements can be written as\n",
    "$$\n",
    "\\text{SPMI}(w,c) = M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\text{PMI}(w,c) - \\log k\n",
    "$$\n",
    "and $\\text{PMI}(w,c) = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{\\#(w)\\cdot\\#(c)} \\right)$ is the well-known pointwise mutual information of $(w,c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:** now write your proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (practical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download dataset [enwik8](http://mattmahoney.net/dc/enwik8.zip) of compressed Wikipedia articles and preprocess raw data with Perl script **main_.pl**. This script will clean all unnecessary symbols, make all words to lowercase, and produce only sentences with words.\n",
    "```\n",
    "wget http://mattmahoney.net/dc/enwik8.zip\n",
    "unzip enwik8.zip\n",
    "mkdir data\n",
    "perl main_.pl enwik8 > data/enwik8.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "file = open(\"/workspace/data/enwik8.txt\", \"r\")\n",
    "doclist = [ line for line in file ]\n",
    "docstr = '' . join(doclist)\n",
    "sentences = re.split(r'[.!?]', docstr)\n",
    "sentences = [sentence.split() for sentence in sentences if len(sentence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889156"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enwik 8\n",
    "data = np.loadtxt(\"/workspace/data/enwik8.txt\", dtype=str, delimiter='.')\n",
    "\n",
    "# Create a list of sentences\n",
    "sentences = [data[i].split() for i in data.nonzero()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achilles', 'wrath', 'is', 'terrible', 'and', 'he', 'slays', 'many', 'trojan', 'warriors', 'and', 'allies', 'including', 'priam', 's', 'son', 'lycaon', 'whom', 'achilles', 'had', 'previously', 'captured', 'and', 'sold', 'into', 'slavery', 'but', 'who', 'had', 'been', 'returned', 'to', 'troy']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[1249])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construct the word vocabulary from the obtained sentences which enumerates words which occur more than $r=200$ times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences, r=200):\n",
    "    prevocabulary = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word in prevocabulary:\n",
    "                prevocabulary[word] += 1\n",
    "            else:\n",
    "                prevocabulary[word] = 1\n",
    "\n",
    "    vocabulary = {}\n",
    "    idx = 0\n",
    "    for word in prevocabulary:\n",
    "        if (prevocabulary[word] > r):\n",
    "            vocabulary[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocabulary(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scan the text corpus with sliding window of size $5$ and step $1$ (which corresponds to $L$=2) and construct co-occurrence word-context matrix $D$ with elements $D_{wc}=\\#(w,c)$. Please, ignore words which occur less than $r=200$ times, but include them into the sliding window. Please, see the graphical illustration of the procedure described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sliding window](sliding_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_matrix(sentences, vocabulary, window_size=5):\n",
    "    \"\"\"\n",
    "    Create a co-occurrence matrix D from training corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    dim = len(vocabulary)\n",
    "    D = np.zeros((dim, dim))\n",
    "    s = window_size//2\n",
    "\n",
    "    for sentence in sentences:\n",
    "        l = len(sentence)\n",
    "        for i in range(l):\n",
    "            for j in range(max(0,i-s), min(i+s+1,l)):\n",
    "                if (i != j and (sentence[i] in vocabulary) \\\n",
    "                    and (sentence[j] in vocabulary)):\n",
    "                    c = vocabulary[sentence[j]]\n",
    "                    w = vocabulary[sentence[i]]\n",
    "                    D[c][w] += 1                  \n",
    "    return D.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889156"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = create_corpus_matrix(sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To find good word embeddings, [Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) proposed to find rank-$d$ SVD decomposition of Shifted Positive Pointwise Mutual Information (SPPMI) matrix\n",
    "$$\n",
    "U \\Sigma V^\\top \\approx \\text{SPPMI}(w,c) = \\max\\left(\\text{SPMI}(w,c), 0 \\right)\n",
    "$$\n",
    "and use $W=U\\sqrt{\\Sigma}$ as word embedding matrix. Your task is to reproduce their results. Write function constructs SPPMI matrix, computes its SVD and produces word-vectors matrix $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sppmi(D, k):\n",
    "    w_ = D.sum(axis=1)\n",
    "    c_ = D.sum(axis=0)\n",
    "    P = D.sum()\n",
    "    w_v, c_v = np.meshgrid(w_, c_)\n",
    "    B = k*(w_v*c_v)/float(P)\n",
    "    #SPPMI = np.maximum(np.log(D)-np.log(B), 0)\n",
    "    SPPMI = np.log(D)-np.log(B)\n",
    "    return SPPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(D, k, d=200):\n",
    "    # SVD initialization\n",
    "    SPPMI = compute_sppmi(D, k)\n",
    "    u, s, vt = svds(SPPMI, k=d)\n",
    "    W_svd = u.dot(np.sqrt(np.diag(s))).T\n",
    "    C_svd = np.sqrt(np.diag(s)).dot(vt)\n",
    "    return W_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgrinch/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/Users/alexgrinch/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py:1817: RuntimeWarning: invalid value encountered in maximum\n",
      "  eigvals = np.maximum(eigvals.real, 0)\n",
      "/Users/alexgrinch/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/alexgrinch/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py:1827: RuntimeWarning: invalid value encountered in greater\n",
      "  above_cutoff = (eigvals > cutoff)\n"
     ]
    }
   ],
   "source": [
    "k = 5 # negative sampling parameter\n",
    "W = compute_embeddings(D, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write class WordVectors using provided template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectors:\n",
    "    \n",
    "    def __init__(self, vocabulary, embedding_matrix):\n",
    "        self.vocab = vocabulary\n",
    "        self.W = embedding_matrix\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def word_vector(self, word):\n",
    "        \"\"\" Takes word and returns its word vector.\n",
    "        \"\"\"\n",
    "        if word in self.vocab:\n",
    "            vec = self.W[:,int(self.vocab[word])]\n",
    "            vec = vec\n",
    "        else:\n",
    "            print (\"No such word in vocabulary.\")\n",
    "            vec = None\n",
    "            \n",
    "        return vec\n",
    "    \n",
    "    def nearest_words(self, word, top=10, display=False):\n",
    "        \"\"\" Takes word from the vocabulary and returns its top_n\n",
    "        nearest neighbors in terms of cosine similarity.\n",
    "        \"\"\"\n",
    "\n",
    "        vec = self.word_vector(word)[None, :]\n",
    "\n",
    "        cosines = cosine_similarity(self.W.T, vec)[:, 0]\n",
    "        args = np.argsort(cosines)[::-1]       \n",
    "\n",
    "        nws = []\n",
    "        for i in range(1, top+1):\n",
    "            nws.append((self.inv_vocab[args[i]], round(cosines[args[i]], 3)))\n",
    "            if (display):\n",
    "                print (self.inv_vocab[args[i]], round(cosines[args[i]], 3))\n",
    "        return nws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordVectors(vocab, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dark', 0.0),\n",
       " ('weather', 0.0),\n",
       " ('stations', 0.0),\n",
       " ('college', 0.0),\n",
       " ('station', 0.0),\n",
       " ('c', 0.0),\n",
       " ('airport', 0.0),\n",
       " ('concentration', 0.0),\n",
       " ('trees', 0.0),\n",
       " ('less', 0.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('communism', 0.792),\n",
       " ('anarcho', 0.787),\n",
       " ('capitalism', 0.784),\n",
       " ('socialism', 0.752),\n",
       " ('liberalism', 0.727),\n",
       " ('criticisms', 0.705),\n",
       " ('capitalist', 0.662),\n",
       " ('fascism', 0.565),\n",
       " ('anarchist', 0.527),\n",
       " ('marxist', 0.518)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukraine', 0.456),\n",
       " ('taiwan', 0.453),\n",
       " ('arabia', 0.448),\n",
       " ('expedition', 0.445),\n",
       " ('saudi', 0.444),\n",
       " ('revolt', 0.443),\n",
       " ('philippines', 0.437),\n",
       " ('syria', 0.411),\n",
       " ('conquest', 0.41),\n",
       " ('sweden', 0.406)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"ussr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukraine', 0.671),\n",
       " ('russia', 0.629),\n",
       " ('poland', 0.55),\n",
       " ('belarus', 0.538),\n",
       " ('yugoslavia', 0.538),\n",
       " ('romania', 0.517),\n",
       " ('serbia', 0.507),\n",
       " ('austria', 0.5),\n",
       " ('hungary', 0.466),\n",
       " ('bulgaria', 0.43)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"ussr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hip', 0.625),\n",
       " ('hop', 0.62),\n",
       " ('pop', 0.606),\n",
       " ('solo', 0.561),\n",
       " ('jazz', 0.556),\n",
       " ('funk', 0.545),\n",
       " ('albums', 0.519),\n",
       " ('singles', 0.519),\n",
       " ('punk', 0.506),\n",
       " ('blues', 0.498)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"rap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hop', 0.828),\n",
       " ('hip', 0.817),\n",
       " ('funk', 0.758),\n",
       " ('rock', 0.737),\n",
       " ('punk', 0.706),\n",
       " ('music', 0.676),\n",
       " ('pop', 0.666),\n",
       " ('scene', 0.659),\n",
       " ('band', 0.657),\n",
       " ('jazz', 0.625)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"rap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Calculate top 10 nearest neighbors with corresponding cosine similarities for the words {**numerical, linear, algebra**} and submit them into telegram bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harmonic', 0.48),\n",
       " ('dynamic', 0.479),\n",
       " ('measurement', 0.471),\n",
       " ('precision', 0.468),\n",
       " ('computational', 0.455),\n",
       " ('derivative', 0.445),\n",
       " ('linear', 0.442),\n",
       " ('calculation', 0.44),\n",
       " ('construct', 0.43),\n",
       " ('determining', 0.43)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computation', 0.547),\n",
       " ('mathematical', 0.532),\n",
       " ('calculations', 0.499),\n",
       " ('polynomial', 0.485),\n",
       " ('calculation', 0.473),\n",
       " ('practical', 0.46),\n",
       " ('statistical', 0.456),\n",
       " ('symbolic', 0.455),\n",
       " ('geometric', 0.441),\n",
       " ('simplest', 0.438)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finite', 0.525),\n",
       " ('binary', 0.499),\n",
       " ('integer', 0.497),\n",
       " ('discrete', 0.49),\n",
       " ('infinite', 0.484),\n",
       " ('differential', 0.479),\n",
       " ('geometric', 0.475),\n",
       " ('algorithms', 0.468),\n",
       " ('geometry', 0.465),\n",
       " ('algebra', 0.448)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('differential', 0.759),\n",
       " ('equations', 0.724),\n",
       " ('equation', 0.682),\n",
       " ('continuous', 0.674),\n",
       " ('multiplication', 0.674),\n",
       " ('integral', 0.672),\n",
       " ('algebraic', 0.667),\n",
       " ('vector', 0.654),\n",
       " ('algebra', 0.63),\n",
       " ('inverse', 0.622)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('binary', 0.516),\n",
       " ('arithmetic', 0.508),\n",
       " ('theorem', 0.498),\n",
       " ('calculus', 0.489),\n",
       " ('geometry', 0.473),\n",
       " ('equation', 0.471),\n",
       " ('notation', 0.47),\n",
       " ('equations', 0.46),\n",
       " ('linear', 0.448),\n",
       " ('infinity', 0.427)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"algebra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('geometry', 0.795),\n",
       " ('calculus', 0.73),\n",
       " ('algebraic', 0.716),\n",
       " ('differential', 0.687),\n",
       " ('equations', 0.665),\n",
       " ('equation', 0.648),\n",
       " ('theorem', 0.647),\n",
       " ('topology', 0.634),\n",
       " ('linear', 0.63),\n",
       " ('integral', 0.618)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"algebra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
